# AI Learning Notes (Embeddings, LLMs, RAG)

## âœ… 1. Embeddings + Similarity

- Text embeddings convert sentences into high-dimensional vectors.
- Cosine similarity can measure semantic similarity between two text inputs.

Example:
```text
â€œHow are you?â€ vs â€œWhatâ€™s up?â€ â†’ high similarity
â€œHow are you?â€ vs â€œBananaâ€     â†’ low similarity
```

## âœ… 2. Retrieval-Augmented Generation (RAG)

**RAG** is a design pattern that enhances LLM responses using relevant context from a knowledge base or dataset.  
Instead of relying only on the model's training, it retrieves relevant information and injects it into the prompt dynamically.

---

### ğŸ”„ RAG Workflow

```text
User Query
   â†“
Embed the Query (OpenAI / HuggingFace model)
   â†“
Compare with Vector Store (cosine similarity)
   â†“
Retrieve Top-K Relevant Chunks
   â†“
Construct Prompt:
  - Inject Retrieved Context
  - Append User Question
   â†“
Send to LLM (e.g., GPT-3.5 / 4)
   â†“
LLM Generates Answer Using Both
```

## âœ… 3. Semantic Search with Embeddings

- Embed a list of document snippets
- Embed a user query
- Find the top-matching document using cosine similarity

This demonstrates the **retriever** logic of RAG in isolation.

- We use a **list comprehension** to embed all documents:
  ```python
  [get_embedding(doc) for doc in documents]
  ```

Key functions:
- `get_embedding()` â€“ returns vector for any text
- `cosine_similarity()` â€“ measures meaning distance between two texts
- `find_most_similar()` â€“ finds the top match from a list
- `argmax()` - returns the index of the highest ele in the list, for topK comparison
